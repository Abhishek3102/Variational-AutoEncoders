{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNUTsiXJhlAF6Ykj9/5EQoc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek3102/Variational-AutoEncoders/blob/main/Change_Singers_Voice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYWQKbArmdnl",
        "outputId": "f5285de6-79e8-498c-e67f-bb9afbcf2a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cpu)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install librosa torch torchvision matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCqxxush5E5I",
        "outputId": "ff827313-ab75-4b4f-ac42-e1448b71a841"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Atif_Aslam  Honey_Singh  preprocessed_Atif_Aslam  preprocessed_Honey_Singh  sample_data  ss  ss.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ss.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmR3JwZj3Kuk",
        "outputId": "6a9a4b3d-66b2-4f61-9405-2ffec978bc64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ss.zip\n",
            "   creating: Atif_Aslam/\n",
            "  inflating: Atif_Aslam/Dil Diyan Gallan Tiger Zinda Hai 128 Kbps.mp3  \n",
            "  inflating: Atif_Aslam/Jeene Laga Hoon Bollywood Sing Along - Ramaiya Vastavaiya - Girish Kumar, Sh.mp3  \n",
            "  inflating: Atif_Aslam/MAIN_RANG_SARBOTO_KA...FULL_SONG(128k).mp3  \n",
            "  inflating: Atif_Aslam/Pehli Nazar Main-(Mr-Jatt.com).mp3  \n",
            "  inflating: Atif_Aslam/Piya O Re Piya-(Mr-Jatt.com).mp3  \n",
            "  inflating: Atif_Aslam/Tere Sang Yaara-(Mr-Jatt.com).mp3  \n",
            "  inflating: Atif_Aslam/Tere_Liye_Lyrical_-_Prince__Vivek_Oberoi_&_Aruna_Sheilds__Atif_Aslam,_Shreya_Ghoshal(256k).mp3  \n",
            "  inflating: Atif_Aslam/Tu_Jaane_Na_Lyrical_Video-_Ajab_Prem_Ki_Ghazab_Kahani__Atif_Aslam__Ranbir_Kapoor,_Katrina_Kaif(128k).mp3  \n",
            "   creating: Honey_Singh/\n",
            "  inflating: Honey_Singh/024 Brown Rang (INTERNATIONAL VILLAGER).mp3  \n",
            "  inflating: Honey_Singh/Alcoholic Full Video _ The Shaukeens _ Yo Yo Honey Singh _ Akshay Kumar & Lisa Haydon.mp3  \n",
            "  inflating: Honey_Singh/Blue Eyes Yo Yo Honey Singh 128 Kbps.mp3  \n",
            "  inflating: Honey_Singh/Dheere_Dheere_Se_Meri_Zindagi_Video_Song_(OFFICIAL)_Hrithik_Roshan,_Sonam_Ka.mp3  \n",
            "  inflating: Honey_Singh/Dope Shope International Villager 128 Kbps.mp3  \n",
            "  inflating: Honey_Singh/High Heels - Jaz Dhami, Yo Yo Honey Singh.m4a  \n",
            "  inflating: Honey_Singh/Party All Night Boss 128 Kbps.mp3  \n",
            "  inflating: Honey_Singh/[Songs.PK] Bhoothnath 03 - Party With Bhoothnath.mp3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_path = \"/content/ss.zip\"\n",
        "extract_path = \"/content/ss\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(f\"folder extracted to: {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDT_UxfG3ups",
        "outputId": "76a9bee7-d6d9-41ec-ee96-3a900ead5c2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "folder extracted to: /content/ss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mimetypes\n",
        "mime_type, encoding = mimetypes.guess_type(zip_path)\n",
        "print(mime_type)  # This should print \"application/zip\" if it's a valid ZIP file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG9uK9MN52cZ",
        "outputId": "3c26f8e3-ec83-4559-f6d7-3969e4613e9f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "application/zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "IHTrjbId6Wys"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(64 * 64 * 64, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(64 * 64 * 64, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "class AudioDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 64 * 64 * 64)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.fc(z)\n",
        "        z = z.view(z.size(0), 64, 64, 64)\n",
        "        return self.conv(z)\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = AudioEncoder(latent_dim)\n",
        "        self.decoder = AudioDecoder(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std"
      ],
      "metadata": {
        "id": "TngjgLb3FgHd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, spectrogram_dir, target_length=29049):\n",
        "        self.files = [os.path.join(spectrogram_dir, f) for f in os.listdir(spectrogram_dir) if f.endswith('.npy')]\n",
        "        self.target_length = target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spectrogram = np.load(self.files[idx])\n",
        "\n",
        "        current_length = spectrogram.shape[-1]\n",
        "\n",
        "        if current_length < self.target_length:\n",
        "            pad_width = self.target_length - current_length\n",
        "            spectrogram = np.pad(spectrogram, ((0, 0), (0, 0), (0, pad_width)), mode='constant')\n",
        "        elif current_length > self.target_length:\n",
        "            spectrogram = spectrogram[:, :, :self.target_length]\n",
        "\n",
        "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min())\n",
        "\n",
        "        spectrogram = torch.tensor(spectrogram).unsqueeze(0).float()\n",
        "\n",
        "        print(f\"Spectrogram shape after padding/truncating: {spectrogram.shape}\")\n",
        "        return spectrogram"
      ],
      "metadata": {
        "id": "Hlw7DaxyGxMl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "def preprocess_audio(input_dir, output_dir, target_duration_sec=120, sr=22050):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    target_samples = target_duration_sec * sr\n",
        "\n",
        "    for file in os.listdir(input_dir):\n",
        "        if file.endswith(('.mp3', '.wav')):\n",
        "            file_path = os.path.join(input_dir, file)\n",
        "            y, _ = librosa.load(file_path, sr=sr)\n",
        "\n",
        "            current_length = len(y)\n",
        "\n",
        "            if current_length < target_samples:\n",
        "                pad_length = target_samples - current_length\n",
        "                y = np.pad(y, (0, pad_length), 'constant')\n",
        "            elif current_length > target_samples:\n",
        "                y = y[:target_samples]\n",
        "\n",
        "            output_file_path = os.path.join(output_dir, file)\n",
        "            sf.write(output_file_path, y, sr)\n",
        "\n",
        "            print(f\"Processed file: {file} | Original length: {current_length / sr:.2f} sec | Target length: {target_samples / sr:.2f} sec\")\n",
        "preprocess_audio('/content/Honey_Singh', 'preprocessed_Honey_Singh', target_duration_sec=120, sr=22050)\n",
        "preprocess_audio('/content/Atif_Aslam', 'preprocessed_Atif_Aslam', target_duration_sec=120, sr=22050)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwMiuWl8Jjv3",
        "outputId": "164edc92-d405-45cd-a3d3-124c2037212f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file: Dheere_Dheere_Se_Meri_Zindagi_Video_Song_(OFFICIAL)_Hrithik_Roshan,_Sonam_Ka.mp3 | Original length: 304.00 sec | Target length: 120.00 sec\n",
            "Processed file: Alcoholic Full Video _ The Shaukeens _ Yo Yo Honey Singh _ Akshay Kumar & Lisa Haydon.mp3 | Original length: 194.44 sec | Target length: 120.00 sec\n",
            "Processed file: Dope Shope International Villager 128 Kbps.mp3 | Original length: 188.61 sec | Target length: 120.00 sec\n",
            "Processed file: [Songs.PK] Bhoothnath 03 - Party With Bhoothnath.mp3 | Original length: 321.46 sec | Target length: 120.00 sec\n",
            "Processed file: 024 Brown Rang (INTERNATIONAL VILLAGER).mp3 | Original length: 179.17 sec | Target length: 120.00 sec\n",
            "Processed file: Party All Night Boss 128 Kbps.mp3 | Original length: 282.82 sec | Target length: 120.00 sec\n",
            "Processed file: Blue Eyes Yo Yo Honey Singh 128 Kbps.mp3 | Original length: 221.00 sec | Target length: 120.00 sec\n",
            "Processed file: Pehli Nazar Main-(Mr-Jatt.com).mp3 | Original length: 311.88 sec | Target length: 120.00 sec\n",
            "Processed file: Tere_Liye_Lyrical_-_Prince__Vivek_Oberoi_&_Aruna_Sheilds__Atif_Aslam,_Shreya_Ghoshal(256k).mp3 | Original length: 301.00 sec | Target length: 120.00 sec\n",
            "Processed file: Tu_Jaane_Na_Lyrical_Video-_Ajab_Prem_Ki_Ghazab_Kahani__Atif_Aslam__Ranbir_Kapoor,_Katrina_Kaif(128k).mp3 | Original length: 337.25 sec | Target length: 120.00 sec\n",
            "Processed file: Dil Diyan Gallan Tiger Zinda Hai 128 Kbps.mp3 | Original length: 260.55 sec | Target length: 120.00 sec\n",
            "Processed file: Jeene Laga Hoon Bollywood Sing Along - Ramaiya Vastavaiya - Girish Kumar, Sh.mp3 | Original length: 201.11 sec | Target length: 120.00 sec\n",
            "Processed file: Piya O Re Piya-(Mr-Jatt.com).mp3 | Original length: 292.08 sec | Target length: 120.00 sec\n",
            "Processed file: Tere Sang Yaara-(Mr-Jatt.com).mp3 | Original length: 290.94 sec | Target length: 120.00 sec\n",
            "Processed file: MAIN_RANG_SARBOTO_KA...FULL_SONG(128k).mp3 | Original length: 263.22 sec | Target length: 120.00 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, audio_dir, target_length=29049, sr=22050):\n",
        "        self.audio_dir = audio_dir\n",
        "        self.target_length = target_length\n",
        "        self.sr = sr\n",
        "        self.files = [f for f in os.listdir(audio_dir) if f.endswith(('.mp3', '.wav'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.audio_dir, self.files[idx])\n",
        "        y, sr = librosa.load(file_path, sr=self.sr)\n",
        "\n",
        "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512)\n",
        "\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        if log_mel_spec.shape[1] < self.target_length:\n",
        "            pad_width = self.target_length - log_mel_spec.shape[1]\n",
        "            log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        elif log_mel_spec.shape[1] > self.target_length:\n",
        "            log_mel_spec = log_mel_spec[:, :self.target_length]\n",
        "\n",
        "        return torch.tensor(log_mel_spec, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(32 * 32 * 7263, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(32 * 32 * 7263, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "class AudioDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 32 * 32 * 7263)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduce filters\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduce filters\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.fc(z)\n",
        "        z = z.view(z.size(0), 32, 32, 7263)\n",
        "        return self.conv(z)\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = AudioEncoder(latent_dim)\n",
        "        self.decoder = AudioDecoder(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=0.001):\n",
        "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + beta * kl_div\n",
        "\n",
        "singer_A_dataset = AudioDataset('preprocessed_Honey_Singh', target_length=29049)\n",
        "singer_B_dataset = AudioDataset('preprocessed_Atif_Aslam', target_length=29049)\n",
        "\n",
        "dataset = torch.utils.data.ConcatDataset([singer_A_dataset, singer_B_dataset])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "device = xm.xla_device()\n",
        "vae = VAE(latent_dim=64).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, spectrogram in enumerate(dataloader):\n",
        "        spectrogram = spectrogram.to(device)\n",
        "\n",
        "\n",
        "        recon, mu, logvar = vae(spectrogram)\n",
        "\n",
        "        loss = vae_loss(recon, spectrogram, mu, logvar)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % 4 == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
        "    metrics = met.metrics_report()\n",
        "    print(f\"TPU Metrics: {metrics}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "zKhEkvkxKXWR",
        "outputId": "2e6c18b8-8572-42e1-a011-5564d35ade1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 1.77G. That was not possible. There are 370.19M free.; (0x0x0_HBM0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-86fc89558b43>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# Move model to TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reduced latent_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 1.77G. That was not possible. There are 370.19M free.; (0x0x0_HBM0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pI7HV_IIHpVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}